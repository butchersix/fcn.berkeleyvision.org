{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "from os.path import isfile, join\n",
    "import ast\n",
    "import re\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the emotions.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = 'emotions.csv'\n",
    "resume_flag = False\n",
    "if isfile('ssl_emotions.csv'):\n",
    "    file = 'ssl_emotions.csv'\n",
    "    resume_flag = True\n",
    "emotions = pd.read_csv(file, encoding='latin-1')\n",
    "emotions.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export log functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_SECONDS = 0\n",
    "session_count = 0\n",
    "\n",
    "def trackSession():\n",
    "    global session_count\n",
    "    if session_count == 0:\n",
    "        session_count += 1\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def setSession(file, flag=True):\n",
    "    now = datetime.datetime.now()\n",
    "    if flag:\n",
    "        file.write(\"\\n-------------------- SESSION - {} -------------------------\\n\".format(now.strftime(\"%Y-%m-%d %H:%M\")))\n",
    "    else:\n",
    "        file.write(\"-------------------- SESSION - {} -------------------------\\n\".format(now.strftime(\"%Y-%m-%d %H:%M\")))\n",
    "\n",
    "def endSession(flag=True):\n",
    "    now = datetime.datetime.now()\n",
    "    if flag:\n",
    "        return \"\\n-------------------- END SESSION - {} -------------------------\\n\".format(now.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "    else:\n",
    "        return \"-------------------- END SESSION - {} -------------------------\\n\".format(now.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "    return \"\"\n",
    "\n",
    "def delayPrint(string, seconds, f=\"clf_logs.log\"): # n seconds delay printing\n",
    "    time.sleep(seconds)\n",
    "    exportLogs(string, f)\n",
    "    print(string)\n",
    "\n",
    "def exportLogs(logs, f=\"clf_logs.log\"):\n",
    "    logs += \"\\n\"\n",
    "    if(isfile(f)):\n",
    "        file = open(f, \"a\")\n",
    "        if trackSession():\n",
    "            setSession(file)\n",
    "        file.write(logs)\n",
    "        file.close()\n",
    "    else:\n",
    "        print(\"Log file does not exist!\")\n",
    "        print(\"Creating {} file...\".format(f))\n",
    "        file = open(f, \"a+\")\n",
    "        if trackSession():\n",
    "            setSession(file)\n",
    "        file.write(logs)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initCtr(iter_file=\"iteration_counter.txt\"):\n",
    "    delayPrint(\"Checking {} file if empty...\".format(iter_file), PRINT_SECONDS)\n",
    "    with open(iter_file, \"r+\") as file:\n",
    "        num_lines = sum(1 for line in file)\n",
    "        if num_lines == 0:\n",
    "            delayPrint(\"Initializing {} file...\".format(iter_file), PRINT_SECONDS)\n",
    "            file.write(\"0\")\n",
    "            delayPrint(\"Done initializing {}\".format(iter_file), PRINT_SECONDS)\n",
    "        else:\n",
    "            delayPrint(\"{} not empty...\".format(iter_file), PRINT_SECONDS)\n",
    "\n",
    "def initTimeData(time_labels, time_file=\"time.txt\"):\n",
    "    delayPrint(\"Checking {} file if empty...\".format(time_file), PRINT_SECONDS)\n",
    "    with open(time_file, \"r+\") as file:\n",
    "        num_lines = sum(1 for line in file)\n",
    "        if num_lines == 0:\n",
    "            delayPrint(\"Initializing {} file...\".format(time_file), PRINT_SECONDS)\n",
    "            for x in time_labels:\n",
    "                file.write(\"{}: []\\n\".format(x))\n",
    "            delayPrint(\"Done initializing {}\".format(time_file), PRINT_SECONDS)\n",
    "        else:\n",
    "            delayPrint(\"{} not empty...\".format(time_file), PRINT_SECONDS)\n",
    "\n",
    "# load time.txt file for resuming purposes\n",
    "def getTimeData(time_file=\"time.txt\"):\n",
    "    time_labels = []\n",
    "    time_data = []\n",
    "    with open(time_file, \"r+\") as file:\n",
    "        delayPrint(\"Reading {} file...\".format(time_file), PRINT_SECONDS)\n",
    "        data = file.readlines()\n",
    "        num_lines = len(data)\n",
    "        for x in range(num_lines):\n",
    "            dt = list(filter(None, re.split(\"(:\\W)\", data[x].rstrip()))) # filter removes blank strings in data[x] list\n",
    "            time_label = dt[0]\n",
    "            time_dt = ast.literal_eval(dt[len(dt) - 1])\n",
    "            time_labels.append(time_label)\n",
    "            time_data.append(time_dt)\n",
    "            delayPrint(\"Fetched {} list...\".format(time_label), PRINT_SECONDS)\n",
    "    return time_labels, time_data\n",
    "\n",
    "def saveTimeData(time_labels, time_data, time_file=\"time.txt\"):\n",
    "    with open(time_file, \"w+\") as file:\n",
    "        # print(\"Time Labels: {} Time Data: {}\".format(time_labels, time_data))\n",
    "        if len(time_labels) == len(time_data):\n",
    "            for x in range(len(time_labels)):\n",
    "                delayPrint(\"Saving time data...\", PRINT_SECONDS)\n",
    "                file.write(\"{} : {}\\n\".format(time_labels[x], time_data[x]))\n",
    "            delayPrint(\"Saved time data...\", PRINT_SECONDS)\n",
    "\n",
    "def initScores(score_labels, score_file=\"scores.txt\"):\n",
    "    delayPrint(\"Checking {} file if empty...\".format(score_file), PRINT_SECONDS)\n",
    "    with open(score_file, \"r+\") as file:\n",
    "        num_lines = sum(1 for line in file)\n",
    "        if num_lines == 0:\n",
    "            delayPrint(\"Initializing {} file...\".format(score_file), PRINT_SECONDS)\n",
    "            for x in score_labels:\n",
    "                file.write(\"{}: []\\n\".format(x))\n",
    "            delayPrint(\"Done initializing {}\".format(score_file), PRINT_SECONDS)\n",
    "        else:\n",
    "            delayPrint(\"{} not empty...\".format(score_file), PRINT_SECONDS)\n",
    "\n",
    "# load scores.txt file for resuming purposes\n",
    "def getScores(score_file=\"scores.txt\"):\n",
    "    score_labels = []\n",
    "    scores = []\n",
    "    with open(score_file, \"r+\") as file:\n",
    "        delayPrint(\"Reading {} file...\".format(score_file), PRINT_SECONDS)\n",
    "        data = file.readlines()\n",
    "        num_lines = len(data)\n",
    "        for x in range(num_lines):\n",
    "            dt = list(filter(None, re.split(\"(:\\W)\", data[x].rstrip()))) # filter removes blank strings in data[x] list\n",
    "            score_label = dt[0]\n",
    "            score = ast.literal_eval(dt[len(dt) - 1])\n",
    "            score_labels.append(score_label)\n",
    "            scores.append(score)\n",
    "            delayPrint(\"Fetched {} list...\".format(score_label), PRINT_SECONDS)\n",
    "    return score_labels, scores\n",
    "\n",
    "def saveScores(score_labels, scores, score_file=\"scores.txt\"):\n",
    "    with open(score_file, \"w+\") as file:\n",
    "        # print(\"Score Labels: {} Scores: {}\".format(score_labels, scores))\n",
    "        if len(score_labels) == len(scores):\n",
    "            for x in range(len(score_labels)):\n",
    "                delayPrint(\"Saving scores...\", PRINT_SECONDS)\n",
    "                file.write(\"{} : {}\\n\".format(score_labels[x], scores[x]))\n",
    "            delayPrint(\"Saved scores...\", PRINT_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove filename column and change column headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not resume_flag:\n",
    "    emotions = emotions.drop('filename', axis=1)\n",
    "    emotions.columns = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog',\n",
    "           'horse', 'motorbike', 'person', 'potted plant', 'sheep', 'sofa',\n",
    "           'train', 'tv/monitor', 'red', 'n_red', 'yellow',\n",
    "           'n_yellow', 'green', 'n_green', 'cyan',\n",
    "           'n_cyan', 'blue', 'n_blue', 'magenta',\n",
    "           'n_magenta', 'emotion']\n",
    "else:\n",
    "    emotions = emotions.drop(\"Unnamed: 0\", axis=1)\n",
    "emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emotions = emotions[:616]\n",
    "# emotions = emotions[:30]\n",
    "# emotions['emotion'][100:len(emotions)] = np.nan\n",
    "# emotions_c = pd.DataFrame()\n",
    "emotions_c = emotions.copy()\n",
    "emotions_c['emotion'] = \"\"\n",
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with NaN values and segregate initial dataframes with labeled and unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emotions_labeled = emotions.dropna()\n",
    "\n",
    "# fix wrong spelling\n",
    "emotions_labeled = emotions_labeled.replace([\"aniticipation\"], \"anticipation\")\n",
    "# emotions_labeled.at[0, 'emotion'] = \"joy\"\n",
    "# emotions_labeled.at[1, 'emotion'] = \"joy\"\n",
    "# emotions_labeled.at[3, 'emotion'] = \"anger\"\n",
    "# emotions_labeled.at[5, 'emotion'] = \"disgust\"\n",
    "# emotions_labeled.at[6, 'emotion'] = \"surprise\"\n",
    "# emotions_labeled.at[8, 'emotion'] = \"trust\"\n",
    "# emotions_labeled.at[9, 'emotion'] = \"trust\"\n",
    "# emotions_labeled.at[13, 'emotion'] = \"disgust\"\n",
    "emotions_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values with empty string\n",
    "# reference: https://stackoverflow.com/questions/13851535/how-to-delete-rows-from-a-pandas-dataframe-based-on-a-conditional-expression\n",
    "emotions_unlabeled = emotions.replace([np.nan], \"\")\n",
    "emotions_unlabeled = emotions_unlabeled.drop(emotions_unlabeled[emotions_unlabeled['emotion'] != \"\"].index)\n",
    "\n",
    "# adjust indices for the unlabeled set\n",
    "emotions_unlabeled.index = range(len(emotions_unlabeled.index))\n",
    "emotions_unlabeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set data X and target y for the labeled data and the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labeled, y_labeled = emotions_labeled.drop('emotion', axis=1), emotions_labeled['emotion']\n",
    "X_unlabeled, y_unlabeled = emotions_unlabeled.drop('emotion', axis=1), emotions_unlabeled['emotion']\n",
    "y_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_labeled, X_test_labeled, y_train_labeled, y_test_labeled = train_test_split(X_labeled, y_labeled, random_state=0)\n",
    "X_train_labeled, X_test_labeled, y_train_labeled, y_test_labeled = train_test_split(X_labeled, y_labeled, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resampling Methods (Oversampling) - Balancing the dataset\n",
    "1. Resample (oversampling) base dataset for base learner\n",
    "2. Resample current dataset of every iteration in the process of fitting\n",
    "3. Resample predicted dataset after SSL for final classifier\n",
    "\n",
    "**CAUTION**: Overfitting is more likely to occur!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X_train_labeled, y_train_labeled)\n",
    "# X_smote_c = pd.DataFrame(X_smote, columns=X_.columns).copy()\n",
    "# X_smote_c['emotion'] = pd.Series(y_smote)\n",
    "delayPrint(\"Normal Distribution: {}\".format(Counter(sorted(y_train_labeled))), PRINT_SECONDS)\n",
    "delayPrint(\"Smote Distribution: {}\".format(Counter(sorted(y_smote))), PRINT_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whole process of constructing labeled data from predicted labels (Semi-supervised learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# only use first default hyperparameters for experimenting only\n",
    "\n",
    "# set hyperparameters of classifier\n",
    "param_grid = {'n_estimators' : [100, 200, 500],\n",
    "              'criterion' : ['gini', 'entropy'],\n",
    "              'max_depth' : [1, 2],\n",
    "              'min_samples_leaf' : [1, 2, 3],\n",
    "              'max_features' : [\"auto\", \"sqrt\", \"log2\", 0.9, 0.2],\n",
    "              'oob_score' : [True],\n",
    "              'n_jobs' : [-1],\n",
    "              'random_state' : [42]}\n",
    "\n",
    "# rfc = GridSearchCV(RandomForestClassifier(), param_grid, cv=10)\n",
    "# rfc = RandomForestClassifier(n_estimators=500,\n",
    "#                              criterion='entropy',\n",
    "#                              max_leaf_nodes=16,\n",
    "#                              n_jobs=-1,\n",
    "#                              random_state=0)\n",
    "\n",
    "# flag for using only base hyperparameters for all iterations\n",
    "isUseBaseParams = False\n",
    "\n",
    "time_file = \"time.txt\"\n",
    "scores_file = \"scores.txt\"\n",
    "\n",
    "# time lists\n",
    "time_labels = [\"Elapsed Time Iteration\", \"Time Passed\"]\n",
    "initTimeData(time_labels)\n",
    "time_labels, time_data = getTimeData(time_file)\n",
    "elapsed_time_list = []\n",
    "elapsed_time_iter_list = time_data[0]\n",
    "time_passed_list = time_data[1]\n",
    "\n",
    "# scores lists\n",
    "score_labels = [\"Best Test Scores\", \"Best Cross-Validation Scores\", \"Out-of-bag Scores\"]\n",
    "initScores(score_labels)\n",
    "score_labels, scores = getScores(scores_file)\n",
    "best_score_list = scores[0]\n",
    "best_cross_val_score = scores[1]\n",
    "oob_score_list = scores[2]\n",
    "\n",
    "iteration_counter = 0\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# print previous session\n",
    "log_file = \"clf_logs.log\"\n",
    "if isfile(log_file):\n",
    "    with open(log_file, \"r+\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "            \n",
    "delayPrint(\"---------- Start Time - {:s} ----------\".format(str(start_time)), PRINT_SECONDS)\n",
    "\n",
    "# define threshold parameter\n",
    "threshold = 0.05\n",
    "\n",
    "# compute number of iterations produced\n",
    "emotions_labeled_len = len(emotions_labeled)\n",
    "emotions_unlabeled_len = len(emotions_unlabeled)\n",
    "while(emotions_labeled_len < len(emotions)):\n",
    "    iteration_counter += 1\n",
    "    estimated_rows = 1 if int((threshold*emotions_unlabeled_len)) < 1 else int((threshold*emotions_unlabeled_len))\n",
    "    emotions_labeled_len += estimated_rows\n",
    "    emotions_unlabeled_len -= estimated_rows\n",
    "\n",
    "# show total number of models\n",
    "num_models_settings = 1\n",
    "for k, v in param_grid.items():\n",
    "    num_models_settings *= len(v)\n",
    "total_num_models = iteration_counter * num_models_settings\n",
    "delayPrint(\"Number of iterations: {}\".format(iteration_counter), PRINT_SECONDS)\n",
    "delayPrint(\"Total number of models: {}\".format(total_num_models), PRINT_SECONDS)\n",
    "\n",
    "# set default iteration_counter\n",
    "iteration_counter_file = \"iteration_counter.txt\"\n",
    "initCtr()\n",
    "    \n",
    "# check saved interation_counter\n",
    "with open(iteration_counter_file, \"r+\") as file:\n",
    "    data = file.readlines()\n",
    "    iteration_counter = int(data[0])\n",
    "    \n",
    "# loop if not all target values have emotions\n",
    "# loop until everything is labeled\n",
    "while(emotions['emotion'].isnull().values.any()):\n",
    "    start_time_iter = datetime.datetime.now()\n",
    "    \n",
    "    # check saved interation_counter\n",
    "    with open(iteration_counter_file, \"r+\") as file:\n",
    "        data = file.readlines()\n",
    "        iteration_counter = int(data[0])\n",
    "    \n",
    "    # show remaining number of models to produce\n",
    "    num_models_current_settings = 1\n",
    "    for k, v in param_grid.items():\n",
    "        num_models_current_settings *= len(v)\n",
    "    current_num_models = iteration_counter * num_models_current_settings\n",
    "    rem_num_models = total_num_models - current_num_models\n",
    "    delayPrint(\"Current number of models: {}\".format(current_num_models), PRINT_SECONDS)\n",
    "    delayPrint(\"Remaining number of models: {}\".format(rem_num_models), PRINT_SECONDS)\n",
    "    \n",
    "    # incrementing iteration_counter\n",
    "    iteration_counter += 1\n",
    "        \n",
    "    # grid search for random forest with 2 standard cross-validation\n",
    "    # original cross-validation = 10\n",
    "    model_file = 'ssl_rf_{}.pickle'.format(threshold*100) # model file name according to threshold percentage\n",
    "    gs_file = 'ssl_gs_rf_{}.pickle'.format(threshold*100)\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(), param_grid)\n",
    "    if isfile(gs_file):\n",
    "        delayPrint(\"Loading ssl gs model...\", PRINT_SECONDS)\n",
    "        grid_search = pickle.load(open(gs_file, 'rb'))\n",
    "        best_params = grid_search.best_params_\n",
    "    elif not isfile(gs_file):\n",
    "        delayPrint(\"Doing grid search...\", PRINT_SECONDS)\n",
    "        grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=10)\n",
    "        # grid_search.fit(X_train_labeled, y_train_labeled)\n",
    "        grid_search.fit(X_smote, y_smote)\n",
    "        best_params = grid_search.best_params_\n",
    "    \n",
    "    # grid search every iteration if learning is base-iter\n",
    "    if not isUseBaseParams:\n",
    "        if isfile(gs_file):\n",
    "            delayPrint(\"Doing grid search...\", PRINT_SECONDS)\n",
    "            grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=10)\n",
    "            grid_search.fit(X_smote, y_smote)\n",
    "            best_params = grid_search.best_params_\n",
    "    \n",
    "    # saving grid search model\n",
    "    with open(gs_file, 'wb') as file:\n",
    "        delayPrint(\"Saving gs model...\", PRINT_SECONDS)\n",
    "        pickle.dump(grid_search, file)\n",
    "    \n",
    "    # use saved model if there is\n",
    "    rfc = RandomForestClassifier()\n",
    "    if isfile(model_file):\n",
    "        delayPrint(\"Loading model...\", PRINT_SECONDS)\n",
    "        rfc = pickle.load(open(model_file, 'rb'))\n",
    "    elif not isfile(model_file): # else-if for using only best base hyperparameters\n",
    "        # comment else and unindent block to get best hyperparameters in each iteration\n",
    "        # set best base parameters\n",
    "        delayPrint(\"Using best hyperparameters...\", PRINT_SECONDS)\n",
    "        rfc.set_params(**best_params)\n",
    "\n",
    "        # train the classifier\n",
    "        # rfc.fit(X_train_labeled, y_train_labeled)\n",
    "        rfc.fit(X_smote, y_smote)\n",
    "    \n",
    "    # fit every iteration if learning is base-iter\n",
    "    if not isUseBaseParams:\n",
    "        if isfile(model_file):\n",
    "            # set best base parameters\n",
    "            delayPrint(\"Using best hyperparameters...\", PRINT_SECONDS)\n",
    "            rfc.set_params(**best_params)\n",
    "\n",
    "            # train the classifier\n",
    "            # rfc.fit(X_train_labeled, y_train_labeled)\n",
    "            rfc.fit(X_smote, y_smote)\n",
    "    \n",
    "    # save model per iteration\n",
    "    with open(model_file, 'wb') as file:\n",
    "        delayPrint(\"Saving model...\", PRINT_SECONDS)\n",
    "        pickle.dump(rfc, file)\n",
    "\n",
    "    # gather class probabilities for each instance prediction\n",
    "    y_pred_rfc = rfc.predict_proba(X_unlabeled)\n",
    "    \n",
    "    # show performance score per run\n",
    "    y_pred_rfc_labeled = rfc.predict(X_test_labeled)\n",
    "    # print(\"Accuracy score {}: {}\".format(iteration_counter, accuracy_score(y_test_labeled, y_pred_rfc_labeled)))\n",
    "    delayPrint(\"Test set score: {:.2f}\".format(grid_search.score(X_test_labeled, y_test_labeled)), PRINT_SECONDS)\n",
    "    delayPrint(\"Best parameters: {}\".format(grid_search.best_params_), PRINT_SECONDS)\n",
    "    delayPrint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_), PRINT_SECONDS)\n",
    "    delayPrint(\"Out-of-Bag Prediction Score: {:.2f}\".format(rfc.oob_score_), PRINT_SECONDS)\n",
    "    \n",
    "    # store scores in list\n",
    "    best_score_list.append(grid_search.score(X_test_labeled, y_test_labeled))\n",
    "    best_cross_val_score.append(grid_search.best_score_)\n",
    "    oob_score_list.append(rfc.oob_score_)\n",
    "    saveScores(score_labels, [best_score_list, best_cross_val_score, oob_score_list], scores_file)\n",
    "\n",
    "    # get highest class probability and pass it to a list\n",
    "    probas = list(map((lambda x: x.max()), y_pred_rfc))\n",
    "    yprfc = list(y_pred_rfc)\n",
    "\n",
    "    # define threshold parameter\n",
    "    # threshold = 0.05\n",
    "\n",
    "    # tuple of predicted X_instances, y_instances, and score for each instance prediction\n",
    "    probas_indices = list(map((lambda x: (X_unlabeled[x['index']:x['index']+1], rfc.classes_[list(x['probas']).index(max(list(x['probas'])))], max(list(x['probas'])))), [{'index' : i, 'probas' : list(p)} for i, p in enumerate(y_pred_rfc)]))\n",
    "\n",
    "    # sort tuple according to its score value\n",
    "    sorted_probas_indices = sorted(probas_indices, key=itemgetter(2), reverse=True)\n",
    "\n",
    "    # get top emotions from the tuple based on its threshold value\n",
    "    # print(\"Sorted probas indices: {}\".format(len(sorted_probas_indices)))\n",
    "    slice_quantity = int(len(sorted_probas_indices)*threshold)\n",
    "    sorted_probas_indices_threshold = sorted_probas_indices[:1 if slice_quantity < 1 else slice_quantity]\n",
    "\n",
    "    # serpate values for X and y instances of top emotions\n",
    "    # len(sorted_probas_indices_threshold)\n",
    "    topy_emotions = list(map((lambda x: x[1]), sorted_probas_indices_threshold))\n",
    "    # print(y_pred_rfc)\n",
    "    # print(sorted_probas_indices_threshold)\n",
    "    delayPrint(\"{}\".format(topy_emotions), PRINT_SECONDS, \"emotion_distribution/emotion_distribution_{}.log\".format(iteration_counter))\n",
    "    topx_emotions_list = sorted_probas_indices_threshold\n",
    "\n",
    "    # instantiate new DataFrame for accumulating all X instances\n",
    "    topx_emotions = pd.DataFrame()\n",
    "\n",
    "    # gather all X predicted instances\n",
    "    for x in topx_emotions_list:\n",
    "        topx_emotions = topx_emotions.append(x[0])\n",
    "    \n",
    "    # adjust indices for topx_emotions\n",
    "    # topx_emotions.index = range(len(topx_emotions.index))\n",
    "    \n",
    "    # remove topx_emotions from the unlabeled data\n",
    "    # print(list(X_unlabeled.index))\n",
    "    emotions_unlabeled = X_unlabeled.drop(X_unlabeled.index[list(topx_emotions.index)])\n",
    "\n",
    "    # add again the target column of the universal set of unlabeled data\n",
    "    emotions_unlabeled['emotion'] = None\n",
    "\n",
    "    # set proper indices for the unlabeled set\n",
    "    # emotions_unlabeled.index = range(len(emotions_labeled.index), len(emotions_labeled.index) + len(emotions_unlabeled.index))\n",
    "    emotions_unlabeled.index = range(len(emotions_unlabeled.index))\n",
    "    \n",
    "    # add target column from the newly instantiated DataFrame along with its instances\n",
    "    topx_emotions['emotion'] = topy_emotions\n",
    "    top_emotions = topx_emotions\n",
    "\n",
    "    # add the predicted instances DataFrame to the universal set of labeled data\n",
    "    emotions_labeled = pd.concat([emotions_labeled, top_emotions], axis=0)\n",
    "\n",
    "    # fix previous indices to its current position in DataFrame\n",
    "    emotions_labeled.index = range(len(emotions_labeled.index))\n",
    "\n",
    "    # combine universal labeled and unlabeled sets into one\n",
    "    emotions = pd.concat([emotions_labeled, emotions_unlabeled], axis=0)\n",
    "\n",
    "    # adjust indices for universal emotions set\n",
    "    emotions.index = range(len(emotions.index))\n",
    "    \n",
    "    # Set data X and target y for the labeled data and the unlabeled data\n",
    "    X_labeled, y_labeled = emotions_labeled.drop('emotion', axis=1), emotions_labeled['emotion']\n",
    "    X_unlabeled, y_unlabeled = emotions_unlabeled.drop('emotion', axis=1), emotions_unlabeled['emotion']\n",
    "    X_train_labeled, X_test_labeled, y_train_labeled, y_test_labeled = train_test_split(X_labeled, y_labeled, random_state=0)\n",
    "    \n",
    "    # re-sampled training dataset\n",
    "    X_smote, y_smote = SMOTE().fit_sample(X_train_labeled, y_train_labeled)\n",
    "    \n",
    "    # print(\"Len of emotions_labeled: {}\".format(len(emotions_labeled)))\n",
    "    # print(\"Len of emotions_unlabeled: {}\".format(len(emotions_unlabeled)))\n",
    "    # print(\"Len of emotions: {}\".format(len(emotions)))\n",
    "    # print(\"Len of x_labeled:{}\".format(len(X_labeled)))\n",
    "    # print(\"Len of y_labeled:{}\".format(len(y_labeled)))\n",
    "    # print(\"Len of x_unlabeled:{}\".format(len(X_unlabeled)))\n",
    "    # print(\"Len of y_unlabeled:{}\".format(len(y_unlabeled)))\n",
    "    \n",
    "    # show emotion distribution\n",
    "    topy_em_pdser = pd.Series(topy_emotions)\n",
    "    emotions_list = sorted(emotions_labeled['emotion'].unique())\n",
    "    emotions_count_list = []\n",
    "    for x in emotions_list:\n",
    "        emotions_count_iter = len(topy_em_pdser.loc[topy_em_pdser == x])\n",
    "        emotions_count = len(emotions_labeled.loc[emotions_labeled['emotion'] == x])\n",
    "        emotions_count_list.append(emotions_count)\n",
    "        delayPrint(\"Number of {}: {}\".format(x, emotions_count_iter), PRINT_SECONDS, \"emotion_distribution/emotion_distribution_{}.log\".format(iteration_counter))\n",
    "    \n",
    "    # bar graph representation of emotion distribution per iteration\n",
    "    fig = plt.figure()\n",
    "    axes = fig.add_subplot(1, 1, 1)\n",
    "    barlist = axes.bar(range(len(emotions_count_list)), emotions_count_list, color=plt.cm.Pastel1(np.arange(len(emotions_count_list))), tick_label=emotions_list)\n",
    "    axes.legend(barlist, emotions_list)\n",
    "    title = \"Emotions Count {} Bar Graph (Semi-supervised Learning)\".format(iteration_counter)\n",
    "    axes.set_title(title)\n",
    "    axes.set_xlabel(\"Emotions\")\n",
    "    axes.set_ylabel(\"Emotions Count\")\n",
    "    fig.set_size_inches(15, 7)\n",
    "    fig.savefig(\"emotion_distribution/bar_graphs/\"+title+\".png\")\n",
    "    \n",
    "    # time computations\n",
    "    end_time_iter = datetime.datetime.now()\n",
    "    elapsed_time_iter = end_time_iter - start_time_iter\n",
    "    elapsed_time_iter_list.append(str(elapsed_time_iter))\n",
    "    time_passed = end_time_iter - start_time\n",
    "    time_passed_list.append(str(time_passed))\n",
    "    # print(\"elapsed_time_iter_list: {}\".format(elapsed_time_iter_list))\n",
    "    # print(\"time_passed_list: {}\".format(time_passed_list))\n",
    "    saveTimeData(time_labels, [elapsed_time_iter_list, time_passed_list], time_file)\n",
    "    delayPrint(\"Accuracy {} elapsed time: {}\".format(iteration_counter, str(elapsed_time_iter)), PRINT_SECONDS)\n",
    "    delayPrint(\"Time passed: {}\\n\".format(str(time_passed)), PRINT_SECONDS)\n",
    "               \n",
    "    # saving interation_counter\n",
    "    with open(iteration_counter_file, \"w+\") as file:\n",
    "        file.write(\"{}\".format(str(iteration_counter)))\n",
    "    \n",
    "    # save DataFrame per iteration\n",
    "    emotions.to_csv(\"ssl_emotions.csv\", encoding='utf-8')\n",
    "    \n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_time_list.append(elapsed_time)\n",
    "delayPrint(\"---------- End Time - {:s} ----------\".format(str(start_time)), PRINT_SECONDS)\n",
    "delayPrint(\"Elapsed time: {}\".format(elapsed_time), PRINT_SECONDS)\n",
    "\n",
    "# save final DataFrame\n",
    "emotions.to_csv(\"ssl_emotions.csv\", encoding='utf-8')\n",
    "\n",
    "# show DataFrame\n",
    "emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check number of emotions in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_file = \"ssl_rf_{}.pickle\".format(threshold*100)\n",
    "if isfile(model_file):\n",
    "    delayPrint(\"Loading model...\", PRINT_SECONDS)\n",
    "    rfc = pickle.load(open(model_file, 'rb'))\n",
    "    # emotions_list = rfc.classes_\n",
    "    emotions_list = sorted(emotions['emotion'].unique())\n",
    "    emotions_count_list = []\n",
    "    for x in emotions_list:\n",
    "        emotions_count = len(emotions.loc[emotions['emotion'] == x])\n",
    "        emotions_count_list.append(emotions_count)\n",
    "        delayPrint(\"Number of {}: {}\".format(x, emotions_count), PRINT_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotions Count Bar Graph (Post Semi-supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "barlist = axes.bar(range(len(emotions_count_list)), emotions_count_list, color=plt.cm.Pastel1(np.arange(len(emotions_count_list))), tick_label=emotions_list)\n",
    "axes.legend(barlist, emotions_list)\n",
    "title = \"Emotions Count Bar Graph (Post Semi-supervised Learning)\"\n",
    "axes.set_title(title)\n",
    "axes.set_xlabel(\"Emotions\")\n",
    "axes.set_ylabel(\"Emotions Count\")\n",
    "fig.set_size_inches(15, 7)\n",
    "fig.savefig(title+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison check if all newly made labeled set is the same with the base labeled set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emotions_o = emotions.copy()\n",
    "emotions_o['emotion'] = \"\" \n",
    "df = pd.concat([emotions_o, emotions_c])\n",
    "df = df.reset_index(drop=True)\n",
    "df_gpby = df.groupby(list(df.columns))\n",
    "idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n",
    "df.reindex(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_rfc_labeled = rfc.predict(X_labeled)\n",
    "# type(y_pred_rfc_labeled)\n",
    "# y_pred_rfc_labeled\n",
    "# accuracy_score(np.array(y_labeled), y_pred_rfc_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = y_labeled.copy()\n",
    "# y_test[0] = np.nan\n",
    "# for x in range(len())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data X and target y for emotions data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = emotions.drop('emotion', axis=1)\n",
    "y = emotions['emotion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample final dataset (Oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote, y_smote = SMOTE().fit_sample(X_train, y_train)\n",
    "# X_smote_c = pd.DataFrame(X_smote, columns=X_.columns).copy()\n",
    "# X_smote_c['emotion'] = pd.Series(y_smote)\n",
    "delayPrint(\"Normal Distribution: {}\".format(Counter(sorted(y_train))), PRINT_SECONDS)\n",
    "delayPrint(\"Smote Distribution: {}\".format(Counter(sorted(y_smote))), PRINT_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build final model from labeled data and show essential scores to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_param_grid = {'n_estimators' : [100, 200, 500],\n",
    "              'criterion' : ['gini', 'entropy'],\n",
    "              'max_depth' : [1, 2],\n",
    "              'min_samples_leaf' : [1, 2, 3],\n",
    "              'max_features' : [\"auto\", \"sqrt\", \"log2\", 0.9, 0.2],\n",
    "              'oob_score' : [True],\n",
    "              'n_jobs' : [-1],\n",
    "              'random_state' : [42]}\n",
    "final_start_time = datetime.datetime.now()\n",
    "delayPrint(\"Start time: {}\".format(final_start_time), PRINT_SECONDS)\n",
    "final_model_file = \"final_ssl_rf_{}.pickle\".format(threshold*100)\n",
    "final_gs_file = \"final_gs_rf_{}.pickle\".format(threshold*100)\n",
    "final_grid_search = GridSearchCV(RandomForestClassifier(), final_param_grid)\n",
    "if isfile(final_model_file) and isfile(final_gs_file):\n",
    "    delayPrint(\"Loading final rfc model...\", PRINT_SECONDS)\n",
    "    final_rfc = pickle.load(open(final_model_file, 'rb'))\n",
    "    delayPrint(\"Loading final grid search model...\", PRINT_SECONDS)\n",
    "    final_grid_search = pickle.load(open(final_gs_file, 'rb'))\n",
    "else:\n",
    "    final_grid_search = GridSearchCV(RandomForestClassifier(), final_param_grid, cv=10)\n",
    "    # final_grid_search.fit(X_train, y_train)\n",
    "    final_grid_search.fit(X_smote, y_smote)\n",
    "    final_best_params = final_grid_search.best_params_\n",
    "    final_rfc = RandomForestClassifier()\n",
    "    final_rfc.set_params(**final_best_params)\n",
    "    # final_rfc.fit(X_train, y_train)\n",
    "    final_rfc.fit(X_smote, y_smote)\n",
    "final_elapsed_time = datetime.datetime.now() - final_start_time\n",
    "final_rfc_log_file = \"final_rfc_clf.log\"\n",
    "delayPrint(\"Elapsed time: {}\".format(final_elapsed_time), PRINT_SECONDS, final_rfc_log_file)\n",
    "delayPrint(\"Test set score: {:.2f}\".format(final_grid_search.score(X_test, y_test)), PRINT_SECONDS, final_rfc_log_file)\n",
    "delayPrint(\"Best parameters: {}\".format(final_grid_search.best_params_), PRINT_SECONDS, final_rfc_log_file)\n",
    "delayPrint(\"Best cross-validation score: {:.2f}\".format(final_grid_search.best_score_), PRINT_SECONDS, final_rfc_log_file)\n",
    "\n",
    "# save final models\n",
    "if not isfile(final_model_file) or not isfile(final_gs_file):\n",
    "    with open(final_model_file, 'wb') as file:\n",
    "        delayPrint(\"Saving final rfc model...\", PRINT_SECONDS)\n",
    "        pickle.dump(final_rfc, file)\n",
    "    with open(final_gs_file, 'wb') as file:\n",
    "        delayPrint(\"Saving final grid search model...\", PRINT_SECONDS)\n",
    "        pickle.dump(final_grid_search, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 Standard cross-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(final_rfc, X, y, cv=10)\n",
    "# scores\n",
    "delayPrint(\"10 standard cross-fold validation mean score: {}\".format(np.mean(scores)), PRINT_SECONDS)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "y = y.astype(np.str)\n",
    "tree_path = \"trees/emotions_tree{}.dot\"\n",
    "# export all the trees\n",
    "for x in range(len(final_rfc.estimators_)):\n",
    "    estimator = final_rfc.estimators_[x]\n",
    "    export_graphviz(\n",
    "            estimator,\n",
    "            out_file=tree_path.format(x),\n",
    "            feature_names=X.columns,\n",
    "            class_names=y.unique(),\n",
    "            rounded=True,\n",
    "            filled=True\n",
    "        )\n",
    "\n",
    "with open(\"trees/emotions_tree0.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all .dots to .pngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "for x in range(len(final_rfc.estimators_)):\n",
    "    (graph,) = pydot.graph_from_dot_file(tree_path.format(x))\n",
    "    graph.write_png('trees/emotions_tree{}.png'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "# y2 = (y == \"disgust\")\n",
    "y3 = cross_val_predict(final_rfc, X, y, cv=10) # y preds for whole dataset\n",
    "y2 = (y == max(dict(Counter(y3)).items(), key=itemgetter(1))[0]) # get highest count of emotions and use basis for binary values\n",
    "y4 = cross_val_predict(final_rfc, X, y2, cv=10) # binary y\n",
    "delayPrint(classification_report(y, y3, target_names=final_rfc.classes_), PRINT_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check number of emotions in final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isfile(final_model_file):\n",
    "    delayPrint(\"Loading model...\", PRINT_SECONDS)\n",
    "    final_rfc = pickle.load(open(model_file, 'rb'))\n",
    "    emotions_list = list(final_rfc.classes_)\n",
    "    emotions_count_list = []\n",
    "    for x in emotions_list:\n",
    "        emotions_count = len(pd.Series(y3).loc[pd.Series(y3) == x])\n",
    "        emotions_count_list.append(emotions_count)\n",
    "        delayPrint(\"Number of {}: {}\".format(x, emotions_count), PRINT_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotions Count Bar Graph (Post Final Class Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "barlist = axes.bar(range(len(emotions_count_list)), emotions_count_list, color=plt.cm.Pastel1(np.arange(len(emotions_count_list))), tick_label=list(final_rfc.classes_))\n",
    "axes.legend(barlist, emotions_list)\n",
    "title = \"Emotions Count Bar Graph (Post Final Class Model)\"\n",
    "axes.set_title(title)\n",
    "axes.set_xlabel(\"Emotions\")\n",
    "axes.set_ylabel(\"Emotions Count\")\n",
    "fig.set_size_inches(15, 7)\n",
    "fig.savefig(title+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mx = confusion_matrix(y, y3)\n",
    "plt.matshow(conf_mx, cmap=plt.cm.Pastel1.reversed())\n",
    "# code for value per box\n",
    "fmt = 'd'\n",
    "thresh = conf_mx.max() / 2.\n",
    "for i, j in itertools.product(range(conf_mx.shape[0]), range(conf_mx.shape[1])):\n",
    "        plt.text(j, i, format(conf_mx[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"black\" if conf_mx[i, j] > thresh else \"black\")\n",
    "plt.ylabel(\"Predicted Labeled Emotions\")\n",
    "plt.xlabel(\"Predicted Emotions of the Predicted Labeled Emotions\")\n",
    "plt.xticks(range(len(final_rfc.classes_)), final_rfc.classes_)\n",
    "plt.yticks(range(len(final_rfc.classes_)), final_rfc.classes_)\n",
    "plt.colorbar()\n",
    "plt.gcf().set_size_inches(10,10)\n",
    "plt.title(\"Emotions Confusion Matrix\")\n",
    "plt.gcf().savefig(\"Emotions Confusion Matrix.png\")\n",
    "plt.show()\n",
    "# fig, axes = plt.figure()\n",
    "# fig.set_size_inches(30, 20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision vs. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, precision_score, recall_score\n",
    "y_scores = cross_val_predict(final_rfc, X, y2, cv=10)\n",
    "precisions, recalls, thresholds = precision_recall_curve(y2, y_scores)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recall, thresholds):\n",
    "    plt.plot(thresholds,precisions[:-1],\"b--\",label=\"Precision\")\n",
    "    plt.plot(thresholds,recalls[:-1],\"g-\",label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"center left\")\n",
    "    plt.ylim([0,1])\n",
    "    \n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.gcf().set_size_inches(15,10)\n",
    "plt.title(\"Precision vs. Recall\")\n",
    "plt.gcf().savefig(\"PrecVsRec.png\")\n",
    "plt.show()\n",
    "delayPrint(\"Precision Score: {}\".format(precision_score(y2, y_scores)), PRINT_SECONDS)\n",
    "delayPrint(\"Recall Score: {}\".format(recall_score(y2, y_scores)), PRINT_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision against recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall(precisions, recall):\n",
    "    plt.plot(recall,precisions,\"b-\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.xlim([0,1])\n",
    "    plt.ylim([0,1])\n",
    "    \n",
    "plot_precision_recall(precisions, recalls)\n",
    "plt.gcf().set_size_inches(15,10)\n",
    "plt.title(\"Precision Against Recall\")\n",
    "plt.gcf().savefig(\"PrecAgRec.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_probas_forest = cross_val_predict(final_rfc, X, y2, cv=10, method=\"predict_proba\")\n",
    "y_scores_forest = y_probas_forest[:, 1]\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y2, y_scores_forest)\n",
    "auc_score = roc_auc_score(y2, y_scores_forest)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"AUC Score = {}\".format(auc_score))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.gcf().set_size_inches(15,10)\n",
    "plt.title(\"Receiving Operator Curve\")\n",
    "plt.gcf().savefig(\"ROC.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration Time Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "etil = list(map((lambda x: x.split(\":\")[1]+\":\"+\"{:.2f}\".format(float(x.split(\":\")[2]))), elapsed_time_iter_list))\n",
    "yticks_etil = []\n",
    "# for x in range(60):\n",
    "#     for y in range(60):\n",
    "#         for z in range(100):\n",
    "#             yticks_etil.append(\"{:02d}:{:02d}.{:02d}\".format(x, y, z))\n",
    "#             print(\"{:02d}:{:02d}.{:02d}\".format(x, y, z))\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(etil, label=\"Time per Iteration\")\n",
    "axes.legend(loc=\"best\")\n",
    "axes.set_xticks(range(iteration_counter))\n",
    "axes.xaxis.set_major_locator(ticker.MultipleLocator(iteration_counter - 1))\n",
    "xtick_labels = [item.get_text() for item in axes.get_xticklabels()]\n",
    "xtick_labels[1] = '1'\n",
    "xtick_labels[len(xtick_labels) - 2] = iteration_counter\n",
    "axes.set_xticklabels(xtick_labels)\n",
    "axes.set_yticks(etil)\n",
    "axes.invert_yaxis()\n",
    "# axes.tick_params(\n",
    "#     axis='y',\n",
    "#     left=False,      # ticks along the bottom edge are off\n",
    "#     labelleft=False) # labels along the bottom edge are off\n",
    "title = \"Time per Iteration Graph\"\n",
    "axes.set_title(title)\n",
    "axes.set_xlabel(\"No. of Iterations\")\n",
    "axes.set_ylabel(\"Time (Minutes:Seconds.Milliseconds)\")\n",
    "plt.tight_layout()\n",
    "plt.gcf().set_size_inches(20, 10)\n",
    "plt.gcf().savefig(title+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.columns.get_loc('background')\n",
    "# len(X.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.values[:, [32, len(X_train.columns) - 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importances of final classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = []\n",
    "for x in range(len(final_rfc.feature_importances_)):\n",
    "    name = X.columns[x]\n",
    "    name_list.append(name)\n",
    "    delayPrint(\"{}. {}: {:.5f}\".format(x+1, name, final_rfc.feature_importances_[x]), PRINT_SECONDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importances bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "barlist = axes.bar(range(len(final_rfc.feature_importances_)), final_rfc.feature_importances_, color=plt.cm.tab20b(np.arange(len(final_rfc.feature_importances_))), tick_label=list(name_list))\n",
    "axes.legend(barlist, name_list)\n",
    "title = \"Feature Importances Bar Graph (Final Class Model)\"\n",
    "axes.set_title(title)\n",
    "axes.set_xlabel(\"Features\")\n",
    "axes.set_ylabel(\"Feature Importance (%)\")\n",
    "fig.set_size_inches(30, 10)\n",
    "fig.tight_layout()\n",
    "fig.savefig(title+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision boundary plotting for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from mlxtend.plotting import plot_decision_regions\n",
    "# dict_features = {}\n",
    "# for x in X_train.columns:\n",
    "#     if X_train.columns.get_loc(x) > 1:\n",
    "#         dict_features.update({X_train.columns.get_loc(x) : list(X_train[x])})\n",
    "# try_rfc_bp = final_rfc.get_params()\n",
    "# try_rfc = RandomForestClassifier(**try_rfc_bp)\n",
    "# try_y = pd.Series(list(map((lambda x: list(final_rfc.classes_).index(x)), y_train)))\n",
    "# for x in X_train.columns:\n",
    "#     idx = X_train.columns.get_loc(x)\n",
    "#     try_rfc.fit(X_train.values[:, [idx, len(X_train.columns) - 1]], try_y)\n",
    "#     fig = plt.figure()\n",
    "#     axes = plot_decision_regions(X_train.values[:, [idx, len(X_train.columns) - 1]],\n",
    "#                                  np.array(try_y),\n",
    "#                                  clf=try_rfc,\n",
    "#                                  legend=2)\n",
    "#     fig.set_size_inches(15, 10)\n",
    "#     rfdb = \"Random Forest Decision Boundary\"\n",
    "#     plt.title(rfdb + \" Feature {} ({}) Graph\".format(idx + 1, x))\n",
    "#     plt.ylabel(\"Emotion Class Indices\")\n",
    "#     if x.startswith(\"n_\"):\n",
    "#         plt.xlabel(\"No. of Color Instances\")\n",
    "#     else:\n",
    "#         plt.xlabel(\"Feature Proportion Value\")\n",
    "#     figfile = \"Random Forest Decision Boundaries/\" + rfdb + \" Feature {} ({}) Graph.png\".format(idx + 1, x.replace(\"/\", \"or\"))\n",
    "#     if not isfile(figfile):\n",
    "#         plt.gcf().savefig(figfile)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Test Score Iteration Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(best_score_list, label=\"Best Test Score per Iteration\")\n",
    "axes.legend(loc=\"best\")\n",
    "axes.set_xticks(range(iteration_counter))\n",
    "axes.xaxis.set_major_locator(ticker.MultipleLocator(iteration_counter - 1))\n",
    "axes.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "xtick_labels = [item.get_text() for item in axes.get_xticklabels()]\n",
    "xtick_labels[1] = '1'\n",
    "xtick_labels[len(xtick_labels) - 2] = iteration_counter\n",
    "axes.set_xticklabels(xtick_labels)\n",
    "axes.set_yticks(np.arange(0, 1.25, 0.25))\n",
    "title = \"Best Test Score per Iteration Graph\"\n",
    "axes.set_title(title)\n",
    "axes.set_xlabel(\"No. of Iterations\")\n",
    "axes.set_ylabel(\"Score Value\")\n",
    "plt.tight_layout()\n",
    "plt.gcf().set_size_inches(20, 10)\n",
    "plt.gcf().savefig(title+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Cross-validation Scores Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(best_cross_val_score, label=\"Best Cross-Validation Score per Iteration\")\n",
    "axes.legend(loc=\"best\")\n",
    "axes.set_xticks(range(iteration_counter))\n",
    "axes.xaxis.set_major_locator(ticker.MultipleLocator(iteration_counter - 1))\n",
    "axes.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "xtick_labels = [item.get_text() for item in axes.get_xticklabels()]\n",
    "xtick_labels[1] = '1'\n",
    "xtick_labels[len(xtick_labels) - 2] = iteration_counter\n",
    "axes.set_xticklabels(xtick_labels)\n",
    "axes.set_yticks(np.arange(0, 1.25, 0.25))\n",
    "title = \"Best Cross-Validation Score per Iteration Graph\"\n",
    "axes.set_title(title)\n",
    "axes.set_xlabel(\"No. of Iterations\")\n",
    "axes.set_ylabel(\"Score Value\")\n",
    "plt.tight_layout()\n",
    "plt.gcf().set_size_inches(20, 10)\n",
    "plt.gcf().savefig(title+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-Bag Scores Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.plot(oob_score_list, label=\"Out-of-Bag Prediction Score per Iteration\")\n",
    "axes.legend(loc=\"best\")\n",
    "axes.set_xticks(range(iteration_counter))\n",
    "axes.xaxis.set_major_locator(ticker.MultipleLocator(iteration_counter - 1))\n",
    "axes.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "xtick_labels = [item.get_text() for item in axes.get_xticklabels()]\n",
    "xtick_labels[1] = '1'\n",
    "xtick_labels[len(xtick_labels) - 2] = iteration_counter\n",
    "axes.set_xticklabels(xtick_labels)\n",
    "axes.set_yticks(np.arange(0, 1.25, 0.25))\n",
    "title = \"Out-of-Bag Prediction Score per Iteration Graph\"\n",
    "axes.set_title(title)\n",
    "axes.set_xlabel(\"No. of Iterations\")\n",
    "axes.set_ylabel(\"Score Value\")\n",
    "plt.tight_layout()\n",
    "plt.gcf().set_size_inches(20, 10)\n",
    "plt.gcf().savefig(title+\".png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
